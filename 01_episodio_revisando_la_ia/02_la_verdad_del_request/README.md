# 02. La Verdad del Request (CURL)

En el cap√≠tulo anterior, vimos un script de Python que parec√≠a inteligente. Ahora, vamos a ver qu√© estaba ocurriendo realmente en la red.

La librer√≠a `ollama` en Python no es m√°s que un "envoltorio" (*wrapper*) que construye peticiones HTTP. No hay magia, solo texto viajando de un lado a otro.

## üì° Anatom√≠a de una Petici√≥n

Cuando interact√∫as con un LLM, no est√°s "abriendo un canal" de comunicaci√≥n persistente. Est√°s enviando una carta y recibiendo una respuesta.

### El Endpoint
Todo sucede aqu√≠: `POST /api/chat`

### El Payload (JSON)
Esto es lo que tu script de Python construy√≥ en segundo plano:

```JSON
{
  "model": "gemma3:12b",
  "messages": [
    { "role": "system", "content": "..." },
    { "role": "user", "content": "Hola" }
  ],
  "stream": false
}
```

- **model:** A qu√© "cerebro" le est√°s hablando.
- **messages:** La parte m√°s importante. Es una lista de objetos.
- **stream:** Si es `true` (por defecto), la IA responde palabra por palabra (efecto m√°quina de escribir). Si es `false`, espera a terminar la frase para responder todo el JSON junto.

## üß† El Problema de la Memoria (Stateless)

Los servidores de LLM son **Stateless** (Sin Estado). Esto significa que no recuerdan nada entre una petici√≥n y la otra.

Si ejecutas el script `01_request_inicial.sh` dos veces, la IA te responder√° como si fuera la primera vez en ambas ocasiones. (Aunque la respuesta puede variar ligeramente debido a la naturaleza probabil√≠stica del modelo).

### ¬øC√≥mo funciona entonces un chat?
El "truco" es que el cliente (nosotros) debe **re-enviar toda la conversaci√≥n** en cada turno.

1. **Turno 1:** Env√≠as `[Mensaje A]`.
2. **Turno 2:** Env√≠as `[Mensaje A, Respuesta A, Mensaje B]`.
3. **Turno 3:** Env√≠as `[Mensaje A, Respuesta A, Mensaje B, Respuesta B, Mensaje C]`.

Si miras el archivo `02_request_con_historial.sh`, ver√°s c√≥mo el JSON crece. Esto explica por qu√© los chats muy largos eventualmente se vuelven lentos o costosos: est√°s enviando una novela entera solo para decir "gracias".

Aqu√≠ aparece la **Ventana de Contexto**. La IA tiene un l√≠mite de palabras (*tokens*) que puede procesar. Si superas ese l√≠mite, el modelo comienza a "olvidar" el principio de la charla.

## üõ†Ô∏è Ejecuci√≥n

Para ver la respuesta cruda de la IA, ejecuta los scripts de bash.
**Importante:** No olvides configurar `OLLAMA_HOST` si no est√°s en localhost.

```Bash
# 1. El saludo inicial
sh 01_request_inicial.sh
```

```Bash
# 2. La respuesta con contexto acumulado
sh 02_request_con_historial.sh
```

## Reto

Aqu√≠ puedes jugar un poco m√°s con los scripts:
- Prueba otros `system prompt`.
- Elimina algunas entradas del historial JSON.
- ¬øQu√© pasa si el `user` escribe dos veces seguidas?
- ¬øQu√© pasa si el `assistant` escribe dos veces seguidas?

---

## ‚è≠Ô∏è Siguiente Paso

Ya entendimos c√≥mo hablamos con el modelo. Pero, ¬øson todos los modelos iguales? Vamos a ponerlos a prueba.

üîô **[Anterior: Abstracci√≥n con Python](../01_abstraccion_con_python)** | üëâ **[Siguiente: El Zool√≥gico de Modelos](../03_el_zoologico_de_modelos)**
