# 01. AbstracciÃ³n con Python: El Investigador Privado

En este ejemplo, utilizamos la librerÃ­a de Python para interactuar con Ollama. El objetivo es ver cÃ³mo unas pocas lÃ­neas de cÃ³digo pueden ocultar toda la complejidad de la comunicaciÃ³n con el LLM.

El script `main.py` levanta un chat infinito donde una IA (con personalidad de investigador privado) intentarÃ¡ sacarte informaciÃ³n sin que te des cuenta.

## ğŸ§  Â¿QuÃ© estÃ¡ pasando realmente?

Aunque parece que la IA "recuerda" lo que le dices, en realidad el script de Python es quien gestiona la memoria.

Cada vez que tÃº escribes algo, el script toma toda la conversaciÃ³n previa y se la vuelve a enviar a Ollama. Ollama es **stateless** (sin estado); no sabe quiÃ©n eres hasta que le enviamos el historial completo.

### Diagrama de Flujo

```mermaid
graph TD
    User([ğŸ‘¤ Usuario]) -->|1. Escribe Input| Script[ğŸ“œ Script Python]
    
    subgraph "GestiÃ³n de Memoria (Client Side)"
    Script -->|2. Append User Msg| History[(ğŸ—‚ï¸ Lista Historial)]
    History -->|3. EnvÃ­a TODO el historial| Client{ğŸ”Œ Cliente Ollama}
    end
    
    subgraph "Server Side"
    Client -->|POST Request| API[ğŸ¤– Ollama API]
    API -->|PredecciÃ³n de texto| Client
    end
    
    Client -->|4. Retorna Respuesta| Script
    Script -->|5. Append AI Msg| History
    Script -->|6. Muestra Respuesta| User
```

## ğŸ› ï¸ Conceptos Clave del CÃ³digo

- **El Cliente (Client):** Es la abstracciÃ³n. Por debajo, esto no es mÃ¡s que una peticiÃ³n HTTP a `http://localhost:11434/api/chat`.
- **El System Prompt:** Es la "configuraciÃ³n de fÃ¡brica" de esta sesiÃ³n. Define la personalidad.
- **La Lista conversation_history:** Es la "memoria RAM" del chat. Si reinicias el script, la IA olvida todo porque esta lista se borra.

## ğŸš€ CÃ³mo ejecutarlo
AsegÃºrate de tener instaladas las dependencias:

```Bash
pip install ollama
```

### OpciÃ³n A: Ollama en Local
Si tienes Ollama corriendo en tu propia mÃ¡quina:

```Bash
python main.py
```

### OpciÃ³n B: Ollama Remoto
Si tu Ollama corre en un servidor, en Docker, o en otra IP de tu red, usa la variable de entorno `OLLAMA_HOST`. El script estÃ¡ preparado para leerla.

```Bash
export OLLAMA_HOST="192.168.1.50"
python main.py
```

## ğŸ§ª Experimento

Como curiosidad, podrÃ­as probar cambiando el `system_prompt` y ver quÃ© pasa. Te dejo el archivo `main_investigador.py` para que veas cÃ³mo se comporta con otro prompt para trabajar.

TambiÃ©n otra cosa que puedes hacer es eliminar la parte del `conversation_history.append(...)` para ver cÃ³mo "el investigador secreto" pierde la memoria.

---

## â­ï¸ Siguiente Paso

Todo esto parece muy simple gracias a Python, Â¿verdad? Pero, Â¿quÃ© estÃ¡ pasando realmente en los cables?

ğŸ‘‰ **[CapÃ­tulo 02: La Verdad del Request](../02_la_verdad_del_request)**
