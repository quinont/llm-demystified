# 01. Abstracci√≥n con Python: El Investigador Privado

En este ejemplo, utilizamos la librer√≠a de Python para interactuar con Ollama. El objetivo es ver c√≥mo unas pocas l√≠neas de c√≥digo pueden ocultar toda la complejidad de la comunicaci√≥n con el LLM.

El script main.py levanta un chat infinito donde una IA (con personalidad de investigador privado) intentar√° sacarte informaci√≥n sin que te des cuenta.

## üß† ¬øQu√© est√° pasando realmente?

Aunque parece que la IA "recuerda" lo que le dices, en realidad el script de Python es quien gestiona la memoria.

Cada vez que t√∫ escribes algo, el script toma toda la conversaci√≥n previa y se la vuelve a enviar a Ollama. Ollama es "stateless" (sin estado); no sabe qui√©n eres hasta que le enviamos el historial completo.

### Diagrama de Flujo


```mermaid
graph TD
    User([üë§ Usuario]) -->|1. Escribe Input| Script[üìú Script Python]
    
    subgraph "Gesti√≥n de Memoria (Client Side)"
    Script -->|2. Append User Msg| History[(üóÇÔ∏è Lista Historial)]
    History -->|3. Env√≠a TODO el historial| Client{üîå Cliente Ollama}
    end
    
    subgraph "Server Side"
    Client -->|POST Request| API[ü§ñ Ollama API]
    API -->|Predecci√≥n de texto| Client
    end
    
    Client -->|4. Retorna Respuesta| Script
    Script -->|5. Append AI Msg| History
    Script -->|6. Muestra Respuesta| User
```


## üõ†Ô∏è Conceptos Clave del C√≥digo

- El Cliente (Client): Es la abstracci√≥n. Por debajo, esto no es m√°s que una petici√≥n HTTP a http://localhost:11434/api/chat.

- El System Prompt: Es la "configuraci√≥n de f√°brica" de esta sesi√≥n. Define la personalidad.

- La Lista conversation_history: Es la "memoria RAM" del chat. Si reinicias el script, la IA olvida todo porque esta lista se borra.

## üöÄ C√≥mo ejecutarlo
Aseg√∫rate de tener instaladas las dependencias:

```Bash
pip install ollama
```

### Opci√≥n A: Ollama en Local
Si tienes Ollama corriendo en tu propia m√°quina:

```Bash
python main.py
```

### Opci√≥n B: Ollama Remoto
Si tu Ollama corre en un servidor, en Docker, o en otra IP de tu red, usa la variable de entorno OLLAMA_HOST. El script est√° preparado para leerla.

```Bash
export OLLAMA_HOST="192.168.1.50"
python main.py
```

## üß™ Experimento

Como curiosidad, podrias probar cambiando el system_prompt y ver que pasa, te dejo el archivo main_investigador.py para que veas como se comporta con otro prompt para trabajar.

Tambien otra cosa que puedes hacer es eliminar la parte del conversation_history.append(...) para ver como "el investigador secreto" pierde la memoria.

Todo esto parece simple no?, veamos en el proximo capitulo lo que esta pasando por abajo...

