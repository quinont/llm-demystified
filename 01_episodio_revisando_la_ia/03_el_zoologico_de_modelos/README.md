# 03. El ZoolÃ³gico de Modelos (TamaÃ±os y Sabores)

No todas las IAs son iguales. En este capÃ­tulo vamos a correr el **mismo prompt** contra 4 cerebros radicalmente distintos para ver cÃ³mo "piensan" (o alucinan).

## ğŸ¦ Los Tipos de Modelos

### 1. Modelos Base / Chat (General Purpose)
Son los estÃ¡ndar (como Llama 3, Mistral). EstÃ¡n entrenados para entender instrucciones y conversar. Son el "promedio" equilibrado.

### 2. Modelos de Razonamiento (Reasoning)
La nueva ola (como **DeepSeek-R1**). Estos modelos no responden inmediatamente; primero generan una cadena de pensamiento interna (`<think>...`) para verificar su lÃ³gica antes de hablar. Son mÃ¡s lentos, pero mucho mejores en matemÃ¡ticas y lÃ³gica.

### 3. Modelos "Tiny" (Borde/IoT)
Modelos comprimidos (como **Qwen 0.5B** o **Llama 1B**). EstÃ¡n hechos para correr en celulares o Raspberry PIs. Son rÃ¡pidos pero propensos a alucinar.

### 4. Modelos Sin Censura (Uncensored)
Modelos (como **Dolphin**) a los que se les ha eliminado el "Alineamiento de Seguridad". No te darÃ¡n sermones morales si preguntas algo controvertido.

## âš–ï¸ Â¿QuÃ© es la CuantizaciÃ³n? (La analogÃ­a del MP3)
VerÃ¡s nombres como `llama3:8b-instruct-q4_0`.
* **8b**: 8 Billones de parÃ¡metros (el tamaÃ±o del cerebro).
* **q4_0**: CuantizaciÃ³n a 4 bits.

Imagina que el modelo original es un audio WAV sin comprimir (FP16 - 16 bits). Pesa muchÃ­simo, es enorme.

La cuantizaciÃ³n es convertirlo a MP3 (4 bits). Pierdes un *poquito* de calidad (inteligencia), pero el archivo pesa un tercio, corre 3 veces mÃ¡s rÃ¡pido y lo puedes correr en una PC promedio.

---

## ğŸ§ª El Experimento
Ejecuta `benchmark_modelos.py`. Vamos a preguntar un acertijo lÃ³gico simple:

> *"Tengo 3 camisas secÃ¡ndose al sol y tardan 1 hora. Si pongo 6 camisas, Â¿cuÃ¡nto tardan?"*

### Resultados esperados (HipÃ³tesis):
* **Tiny:** Probablemente diga "2 horas" (falla matemÃ¡tica lineal).
* **Large:** DirÃ¡ "1 hora" (entiende el contexto fÃ­sico).
* **Reasoning:** Te mostrarÃ¡ paso a paso su deducciÃ³n fÃ­sica.
* **Uncensored:** ResponderÃ¡ directo y sin rodeos.

**Nota:** Como todos los modelos van a correr en el mismo servidor, la ejecuciÃ³n puede tomar tiempo dependiendo de tu hardware (carga/descarga de modelos en VRAM).

```bash
python benchmark_modelos.py
```

---

## Reto

Siempre es posible ir un poco mÃ¡s a fondo:
- Â¿QuÃ© pasa con otros modelos que conozcas?
- Â¿QuÃ© pasa si ocupas un modelo "reasoning" mÃ¡s pequeÃ±o? Â¿O mÃ¡s grande?

---

## â­ï¸ Siguiente Episodio

Hemos terminado la introducciÃ³n bÃ¡sica. Sabemos hablar con la IA y sabemos que hay diferentes tipos.
Ahora, Â¿quÃ© pasa si hacemos que **dos IAs hablen entre sÃ­**?

ğŸ”™ **[Anterior: La Verdad del Request](../02_la_verdad_del_request)** | ğŸ‘‰ **[Episodio 2: IA Hablando con IA](../../02_ia_hablando_con_ia)**
