# 游 Instalando Ollama

Ollama es el motor que nos permite ejecutar LLMs (Large Language Models) localmente de manera sencilla.

## 1. Instalaci칩n Oficial

Para instalar Ollama, sigue las instrucciones oficiales en su sitio web. Es la fuente m치s actualizada y confiable:

游녤 **[Descargar Ollama](https://ollama.com/download)**

Una vez instalado, verifica que est칠 corriendo abriendo una terminal y ejecutando:

```bash
ollama --version
```

## 2. Descargando los Modelos

Este proyecto utiliza varios modelos espec칤ficos en sus ejemplos. Necesitas descargarlos ("pull") para que los scripts de Python funcionen correctamente.

Ejecuta los siguientes comandos en tu terminal:

```bash
# Modelos base para los ejemplos
ollama pull gemma3:12b
ollama pull qwen3:14b
ollama pull dolphin3:8b
ollama pull ministral-3:14b
```

> **丘멆잺 Nota Importante:**
> Los nombres de los modelos (`gemma3`, `qwen3`, etc.) pueden referirse a versiones muy recientes o espec칤ficas que evolucionan r치pido.
> Si alguno de los comandos falla (ej. "manifest not found"), por favor busca la versi칩n equivalente m치s actual en la [librer칤a de Ollama](https://ollama.com/library) (por ejemplo `gemma2`, `qwen2.5`, `mistral-nemo`) y actualiza la referencia en el c칩digo Python correspondiente.

## 3. Verificaci칩n R치pida

Para asegurarte de que tu instalaci칩n funciona y puedes ejecutar modelos, prueba correr un modelo peque침o (como `gemma2:2b` o el que hayas descargado):

```bash
ollama run gemma2:2b "Hola, 쯘st치s funcionando?"
```

Si recibes una respuesta, 춰est치s listo para continuar!
