# üß† Instalando Ollama

Ollama es el motor que nos permite ejecutar LLMs (Large Language Models) localmente de manera sencilla.

## 1. Instalaci√≥n Oficial

Para instalar Ollama, sigue las instrucciones oficiales en su sitio web. Es la fuente m√°s actualizada y confiable:

üëâ **[Descargar Ollama](https://ollama.com/download)**

Una vez instalado, verifica que est√© corriendo abriendo una terminal y ejecutando:

```bash
ollama --version
```

### ¬øPor qu√© Ollama? (vs OpenAI, Google, Anthropic)

Aunque el c√≥digo de este proyecto utiliza la librer√≠a `ollama` para interactuar con modelos locales, es importante aclarar que **los conceptos que aprender√°s aqu√≠ son universales**.

La estructura de los mensajes (`system`, `user`, `assistant`), el uso de herramientas (*tool calling*), y el manejo de contexto son pr√°cticamente id√©nticos si decidieras usar las APIs de:
- OpenAI (GPT-4o, o1)
- Google (Gemini)
- Anthropic (Claude)
- Groq, Mistral, etc.

**Elegimos Ollama para este curso porque:**
1.  **Es Gratis:** No necesitas tarjetas de cr√©dito ni pagar por tokens para aprender.
2.  **Es Privado:** Tus datos no salen de tu m√°quina.
3.  **Es "Desmitificador":** Al correrlo en tu propia PC, ves que no es magia en la nube, es solo software ejecut√°ndose en tu hardware.

## 2. Descargando los Modelos

Este proyecto utiliza varios modelos espec√≠ficos en sus ejemplos. Necesitas descargarlos ("pull") para que los scripts de Python funcionen correctamente.

Ejecuta los siguientes comandos en tu terminal:

```bash
# Modelos base para los ejemplos
ollama pull gemma3:12b
ollama pull qwen3:14b
ollama pull dolphin3:8b
ollama pull ministral-3:14b
```

> **‚ö†Ô∏è Nota Importante:**
> Los nombres de los modelos (`gemma3`, `qwen3`, etc.) pueden referirse a versiones muy recientes o espec√≠ficas que evolucionan r√°pido.
> Si alguno de los comandos falla (ej. "manifest not found"), por favor busca la versi√≥n equivalente m√°s actual en la [librer√≠a de Ollama](https://ollama.com/library) (por ejemplo `gemma2`, `qwen2.5`, `mistral-nemo`) y actualiza la referencia en el c√≥digo Python correspondiente.

## 3. Verificaci√≥n R√°pida

Para asegurarte de que tu instalaci√≥n funciona y puedes ejecutar modelos, prueba correr un modelo peque√±o (como `gemma2:2b` o el que hayas descargado):

```bash
ollama run gemma2:2b "Hola, ¬øest√°s funcionando?"
```

Si recibes una respuesta, ¬°est√°s listo para continuar!

---
**Navegaci√≥n:**
‚¨ÖÔ∏è [Anterior (Setup Overview)](../README.md) | ‚û°Ô∏è [Siguiente (Entorno Python)](../02_entorno_python/README.md)
