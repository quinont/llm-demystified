# ğŸ§  Instalando Ollama

Ollama es el motor que nos permite ejecutar LLMs (Large Language Models) localmente de manera sencilla.

## 1. InstalaciÃ³n Oficial

Para instalar Ollama, sigue las instrucciones oficiales en su sitio web. Es la fuente mÃ¡s actualizada y confiable:

ğŸ‘‰ **[Descargar Ollama](https://ollama.com/download)**

Una vez instalado, verifica que estÃ© corriendo abriendo una terminal y ejecutando:

```bash
ollama --version
```

### Â¿Por quÃ© Ollama? (vs OpenAI, Google, Anthropic)

Aunque el cÃ³digo de este proyecto utiliza la librerÃ­a `ollama` para interactuar con modelos locales, es importante aclarar que **los conceptos que aprenderÃ¡s aquÃ­ son universales**.

La estructura de los mensajes (`system`, `user`, `assistant`), el uso de herramientas (*tool calling*), y el manejo de contexto son prÃ¡cticamente idÃ©nticos si decidieras usar las APIs de:
- OpenAI (GPT-4o, o1)
- Google (Gemini)
- Anthropic (Claude)
- Groq, Mistral, etc.

**Elegimos Ollama para este curso porque:**
1.  **Es Gratis:** No necesitas tarjetas de crÃ©dito ni pagar por tokens para aprender.
2.  **Es Privado:** Tus datos no salen de tu mÃ¡quina.
3.  **Es "Desmitificador":** Al correrlo en tu propia PC, ves que no es magia en la nube, es solo software ejecutÃ¡ndose en tu hardware.

## 2. Descargando los Modelos

Este proyecto utiliza varios modelos especÃ­ficos en sus ejemplos. Necesitas descargarlos ("pull") para que los scripts de Python funcionen correctamente.

Ejecuta los siguientes comandos en tu terminal:

```bash
# Modelos base para los ejemplos (Versiones actuales recomendadas)
ollama pull gemma2:9b
ollama pull qwen2.5:14b
ollama pull dolphin-mixtral:8x7b
ollama pull mistral-nemo:12b
```

> **âš ï¸ Nota Importante:**
> Los nombres de los modelos (`gemma2`, `qwen2.5`, etc.) evolucionan rÃ¡pido.
> Si alguno de los comandos falla (ej. "manifest not found"), por favor busca la versiÃ³n equivalente mÃ¡s actual en la [librerÃ­a de Ollama](https://ollama.com/library).

## 3. VerificaciÃ³n RÃ¡pida

Para asegurarte de que tu instalaciÃ³n funciona y puedes ejecutar modelos, prueba correr un modelo pequeÃ±o (como `gemma2:2b` o el que hayas descargado):

```bash
ollama run gemma2:2b "Hola, Â¿estÃ¡s funcionando?"
```

Si recibes una respuesta, Â¡estÃ¡s listo para continuar!

---

## ğŸš€ Siguiente paso

Ya tenemos el cerebro listo. Ahora necesitamos un cuerpo (el cÃ³digo) para interactuar con Ã©l. Vamos a configurar el entorno de desarrollo.

â¬…ï¸ [Volver al Inicio](../README.md) | ğŸ‘‰ **[Paso 2: El Cuerpo (Entorno Python)](../02_entorno_python/README.md)**
